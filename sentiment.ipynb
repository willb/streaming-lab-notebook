{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural language processing and sentiment analysis\n",
    "\n",
    "In this notebook, we'll show you how to analyze the sentiments (positive, negative, or neutral) of our synthetic social media updates.  There are a lot of tricky problems in preparing natural language inputs for machine learning and in training machine learning models for natural language, but we're going to ignore those because we can use some libraries that have done most of the hard work for us:\n",
    "\n",
    "1.  [https://spacy.io](spaCy) is a library that has methods and pretrained models for parsing natural language and determining parts of speech, etc., in many languages, and\n",
    "2.  [VADER](https://github.com/cjhutto/vaderSentiment) is a library that can characterize the sentiments of individual sentences.\n",
    "\n",
    "While sentiment analysis is an interesting application, we hope you'll be inspired to try other language-processing tasks with spaCy, too.  You'll also learn how to glue sophisticated language-processing code into a Spark pipeline so that you can use it to process streaming data.  \n",
    "\n",
    "The bigger lesson of this notebook is that you may not always need to train a model to add intelligence to an application:  often pretrained models or even off-the-shelf intelligent APIs are available for interesting tasks (this is particularly true for tasks like language processing and image recognition that are broadly applicable).\n",
    "\n",
    "## Setup\n",
    "\n",
    "We'll start by importing the spaCy library and telling it to load a pretrained model for English text.  The spaCy project ships [several pretrained models](https://spacy.io/models/), but we are going to use [a relatively compact model of English](https://spacy.io/models/en#en_core_web_sm) trained on web pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.cli.download(\"download\", \"en\")\n",
    "english = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to import the analyzer class from VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text and identifying sentiments\n",
    "\n",
    "We'll look at the sentiment of some example text from Jane Austen (we picked a notably recognizable excerpt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampletext = \"\"\" It is a truth universally acknowledged, that a single man in possession\n",
    "of a good fortune, must be in want of a wife.\n",
    "\n",
    "However little known the feelings or views of such a man may be on his\n",
    "first entering a neighbourhood, this truth is so well fixed in the minds\n",
    "of the surrounding families, that he is considered the rightful property\n",
    "of some one or other of their daughters. \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to tell spaCy to use `english` -- the model we loaded -- to parse the input text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = english(sampletext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may not know what we can do with a `spacy.tokens.doc.Doc`, but since most good Python code includes documentation, we can find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the cool things we can do with spaCy is identify parts of speech in natural language text (in fact, we used this feature to identify words that should become hashtags in our [synthetic update generator](/notebooks/generate.ipynb)):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in result:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works pretty well!  But for our purposes in this notebook, we're just going to use spaCy to divide updates into sentences, which we can then feed to VADER.  Let's instantiate a VADER analyzer now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can get the sentiment scores for each sentence:  negative (`neg`), neutral (`neu`), positive (`pos`), and overall sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[analyzer.polarity_scores(str(s)) for s in list(result.sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the first two sentences of _Pride and Prejudice_ score as neutral-to-positive (we don't have a pretrained hilarity detector, alas).  Let's try some raw text from the negative product reviews corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative = english(\"\"\" This oatmeal is not good. Its mushy, soft, I don't like it. Quaker Oats is the way to go. \n",
    "\n",
    "Seriously this product was as tasteless as they come. There are much better tasting products out \n",
    "there but at 100 calories its better than a special k bar or cookie snack pack. You just have to \n",
    "season it or combine it with something else to share the flavor.\n",
    "\n",
    "These were nasty, they were so greasy and too rich for my blood, plus they lacked major flavor, \n",
    "no spicy jalapeno flavor at all.\n",
    "\"\"\")\n",
    "\n",
    "[(s, analyzer.polarity_scores(str(s))) for s in list(negative.sents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Streaming natural language processing\n",
    "\n",
    "Now we'll connect this sort of analysis to Spark so we can apply it to streaming data.  As before, we're going to load the Kafka connector package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "SPARK_VERSION=\"2.3.2\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] --conf spark.jars.ivy=/opt/app-root/src/.ivy2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:%s pyspark-shell\" % SPARK_VERSION\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/app-root/lib/python3.6/site-packages/pyspark\"\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll set up a Spark session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"sentiment test\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we'll deserialize the JSON message payloads into structured data that we can process easily with Spark's data frame operations or structured streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from pyspark.sql.functions import column, from_json\n",
    "\n",
    "structure = StructType([StructField(fn, StringType(), True) for fn in \"text user_id update_id\".split()])\n",
    "\n",
    "records = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"odh-message-bus-kafka-bootstrap:9092\") \\\n",
    "  .option(\"subscribe\", \"social-firehose\") \\\n",
    "  .load() \\\n",
    "  .select(column(\"value\").cast(StringType()).alias(\"value\")) \\\n",
    "  .select(from_json(column(\"value\"), structure).alias(\"json\")) \\\n",
    "  .select(column(\"json.update_id\"), column(\"json.user_id\").alias(\"user_id\"), column(\"json.text\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to use spaCy and VADER in Spark [user-defined functions](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.udf) so that we can \n",
    "\n",
    "1.  write a query that parses longer text in a data frame or structured stream into multiple sentences, and\n",
    "2.  write a query that generates sentiment scores for individual records.\n",
    "\n",
    "Since spaCy depends on a rather large and expensive-to-load model file, we don't want to refer to the model file directly in our user-defined function:  it would be prohibitive to serialize and deserialize it every time we wanted to run the function.  Typically with Spark programs, we'd prefer to [broadcast](https://spark.apache.org/docs/latest/rdd-programming-guide.html#broadcast-variables) large data like models, but the spaCy model is tricky to serialize.  So instead, we'll use this trick suggested by the [Sparkling Pandas library](https://github.com/sparklingpandas/sparklingml/blob/627c8f23688397a53e2e9e805e92a54c2be1cf3d/sparklingml/transformation_functions.py#L53), essentially simulating lazily-initialized worker-local storage for Spacy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is borrowed from Sparkling Pandas; see here:\n",
    "# https://github.com/sparklingpandas/sparklingml/blob/627c8f23688397a53e2e9e805e92a54c2be1cf3d/sparklingml/transformation_functions.py#L53\n",
    "class SpacyMagic(object):\n",
    "    \"\"\"\n",
    "    Simple Spacy Magic to minimize loading time.\n",
    "    >>> SpacyMagic.get(\"en\")\n",
    "    <spacy.en.English ...\n",
    "    \"\"\"\n",
    "    _spacys = {}\n",
    "\n",
    "    @classmethod\n",
    "    def get(cls, lang):\n",
    "        if lang not in cls._spacys:\n",
    "            import spacy\n",
    "            cls._spacys[lang] = spacy.load(lang)\n",
    "        return cls._spacys[lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make a user-defined function to split social-media updates into sentences.  We will use spaCy, which is more expensive than most reasonable heuristics for splitting text into sentences (but also much smarter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "def split_sentences_impl(s):\n",
    "    \"\"\" splits an English string into sentences, using spaCy \"\"\"\n",
    "    english = SpacyMagic.get(\"en\")\n",
    "    return [str(sentence) for sentence in english(s).sents]\n",
    "\n",
    "split_sentences = udf(split_sentences_impl, ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see what this looks like, we'll run it on the first 10 rows of the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_records = records \\\n",
    "  .orderBy(\"update_id\") \\\n",
    "  .limit(10) \\\n",
    "  .select(\"update_id\", \"user_id\", split_sentences(column(\"text\")).alias(\"sentences\")) \\\n",
    "  .cache()\n",
    "\n",
    "split_records.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can explode each array into multiple rows to make further processing easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "sentences = split_records.select(\"update_id\", \"user_id\", explode(column(\"sentences\")).alias(\"sentence\"))\n",
    "sentences.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create our user-defined function for VADER scoring:  it will take text and return a sentiment structure.  Note that we _are_ actually creating a broadcast variable for the VADER model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "sentiment_fields = \"pos neg neu compound\".split()\n",
    "sentiment_structure = StructType([StructField(fn, FloatType(), True) for fn in sentiment_fields])\n",
    "\n",
    "analyzer_bcast = spark.sparkContext.broadcast(analyzer)\n",
    "\n",
    "def vader_impl(s):\n",
    "    va = analyzer_bcast.value\n",
    "    result = va.polarity_scores(s)\n",
    "    return [result[key] for key in sentiment_fields]\n",
    "\n",
    "sentiment_score = udf(vader_impl, sentiment_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can annotate each sentence with its sentiment and order from most negative to most positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences \\\n",
    "  .select(\"update_id\", \"user_id\", \"sentence\", sentiment_score(column(\"sentence\")).alias(\"sentiment\")) \\\n",
    "  .orderBy(\"sentiment.compound\") \\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
