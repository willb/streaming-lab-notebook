{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started processing Kafka with Spark\n",
    "\n",
    "The first thing we'll need to do is tell Spark where to find the Kafka driver before we set Spark up.  Currently, our notebook images are built against Spark 2.3.2.  If you're using this with a different version of Spark, be sure to change `SPARK_VERSION` in the cell below before executing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "SPARK_VERSION=\"2.3.2\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[2] --conf spark.jars.ivy=/opt/app-root/src/.ivy2 --packages org.apache.spark:spark-sql-kafka-0-10_2.11:%s pyspark-shell\" % SPARK_VERSION\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/app-root/lib/python3.6/site-packages/pyspark\"\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we'll connect to Spark by establishing a `SparkSession`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .appName(\"Social Firehose\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to begin by loading the contents of a Kafka topic into a data frame.  Because Spark data frames are _lazy_, or recomputed when accessed, this data frame will always have the most recent collection of messages in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark \\\n",
    "  .read \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"odh-message-bus-kafka-bootstrap:9092\") \\\n",
    "  .option(\"subscribe\", \"social-firehose\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this data frame always has the most recent collection of messages by running the `count()` action on it twice with a short delay in the middle.  Note how many messages are generated in ten seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "a = df.count()\n",
    "time.sleep(10)\n",
    "b = df.count()\n",
    "(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first few messages, but they'll be in a pretty raw format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll import some functions and types from the Spark library so we can do something more useful with our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.functions import column\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.types import StructType\n",
    "from pyspark.sql.types import StructField"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we'll do is extract the JSON payloads of the messages; we'll inspect the first ten as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = df.select(df[\"value\"].cast(StringType()).alias(\"value\"))\n",
    "values.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing we'll do is impose some structure on the messages by converting the serialized JSON objects into actual records:\n",
    "\n",
    "1.  First, we'll declare a `StructType` for the structure of our messages (three strings, named `text`, `user_id`, and `update_id`),\n",
    "2.  Next, we'll convert the JSON strings to structures using the `from_json` dataframe function, and\n",
    "3.  Finally, we'll `SELECT` the fields of the object so we have something that looks like a flat database tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structure = StructType([StructField(fn, StringType(), True) for fn in \"text user_id update_id\".split()])\n",
    "records = values.select(from_json(values[\"value\"], structure).alias(\"json\")) \\\n",
    "                .select(column(\"json.update_id\"), column(\"json.user_id\").alias(\"user_id\"), column(\"json.text\"))\n",
    "records.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can perform database-style aggregations on this data frame, like identifying the users responsible for the most status updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_counts = records.groupBy(\"user_id\").count().orderBy(\"count\", ascending=False)\n",
    "user_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run that query several times with a short delay in between, you may get different results since the data frame will reflect newly-arriving messages.  Try it out!\n",
    "\n",
    "We can also count the number of users who have issued status updates (because of how we're generating the synthetic stream of updates, there is an upper bound on this number):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records.select(\"user_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also identify the most prolix users.  You probably have some social media connections who take advantage of every extra bit of character limit; a query like this will show you who they are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "user_loquacity = records.select(column(\"user_id\"), length(\"text\").alias(\"update_len\")) \\\n",
    "  .groupBy(\"user_id\") \\\n",
    "  .avg() \\\n",
    "  .orderBy(\"avg(update_len)\", ascending=False)\n",
    "user_loquacity.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also identify the most popular hashtags in users' updates.  We'll start by turning each update into an array of words.  Then we'll explode each array into multiple rows, so that each row has a separate, single element, i.e.\n",
    "\n",
    "```\n",
    "1, 2, \"foo bar blah\"\n",
    "```\n",
    "\n",
    "would become\n",
    "\n",
    "```\n",
    "1, 2, [foo, bar, blah]\n",
    "```\n",
    "\n",
    "which would become\n",
    "\n",
    "```\n",
    "1, 2, foo\n",
    "1, 2, bar\n",
    "1, 2, blah\n",
    "```\n",
    "\n",
    "We'll then filter for hashtags (keeping only words starting with `#`) so we can find the most popular!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = records.select(explode(split(\"text\", \" \")).alias(\"word\"))\n",
    "hashtags = words.filter(column(\"word\").startswith(\"#\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_counts = hashtags.groupBy(\"word\").count().orderBy(\"count\", ascending=False)\n",
    "hashtag_counts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
